import streamlit as st
import pandas as pd
import joblib
import plotly.graph_objects as go
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

st.title("üîç Comparateur de Mod√®les de R√©gression")

# --- Chargement des datasets ---
@st.cache_data
def load_data():
    df1 = pd.read_csv("data/learn_model1.csv")
    df2 = pd.read_csv("data/learn_model2.csv")
    df4 = pd.read_csv("data/learn_model4.csv")
    return df1, df2, df4

# --- Chargement des mod√®les et des colonnes associ√©es ---
@st.cache_resource
def load_models():
    model1 = joblib.load("model1.pkl")          # GridSearchCV (pas pipeline)
    model2_rf = joblib.load("model2.pkl")
    model2_reg = joblib.load("modele2_reg.pkl") # Pipeline complet
    model4 = joblib.load("model4.pkl")
    
    # Chargement des colonnes d'entra√Ænement pour les mod√®les sans pipeline
    features1 = joblib.load("model1_features.pkl")
    return model1, model2_rf, model2_reg, model4, features1

# Donn√©es et mod√®les
learn_model1, learn_model2, learn_model4 = load_data()
model1, model2_rf, model2_reg, model4, features1 = load_models()

models_data = {
    "Mod√®le 1 - Random Forest": (learn_model1, model1, features1),
    "Mod√®le 2 - Random Forest": (learn_model2, model2_rf, None),
    "Mod√®le 2 - R√©gression Lin√©aire": (learn_model2, model2_reg, None),
    "Mod√®le 4 - Random Forest": (learn_model4, model4, None)
}

# S√©lecteur
selected = st.selectbox("S√©lectionnez un mod√®le :", list(models_data.keys()))
data, model, custom_features = models_data[selected]

st.subheader(f"Aper√ßu des donn√©es ({selected})")
st.write(data.head())

if 'target' not in data.columns:
    st.error("‚ùå Colonne 'target' absente du dataset.")
else:
    try:
        y = data["target"]

        # Cas des mod√®les avec pipeline complet
        if hasattr(model, "predict") and not hasattr(model, "best_estimator_"):
            X = data.drop(columns=["target", "UNIQUE_ID", "insee_code"], errors="ignore")
        
        # Cas des mod√®les GridSearchCV (sans pipeline)
        elif custom_features is not None:
            X = data[custom_features]
        else:
            X = data.drop(columns=["target", "UNIQUE_ID", "insee_code"], errors="ignore")

        # Pr√©dictions
        y_pred = model.predict(X)

        # Scores
        r2 = r2_score(y, y_pred)
        rmse = np.sqrt(mean_squared_error(y, y_pred))

        st.metric("üìà R¬≤ (sur tout le dataset)", round(r2, 3))
        st.metric("üìâ RMSE", round(rmse, 2))

        # R¬≤ validation crois√©e (si disponible)
        if hasattr(model, "best_score_"):
            st.metric("‚úÖ R¬≤ (validation crois√©e)", round(model.best_score_, 3))
        elif hasattr(model, "cross_val_r2"):
            st.metric("‚úÖ R¬≤ (validation crois√©e)", round(model.cross_val_r2, 3))
        else:
            st.warning("Aucune validation crois√©e disponible pour ce mod√®le.")

        # Graphe interactif
        n = st.slider("Nombre de points √† afficher :", min_value=50, max_value=min(len(y), 1000), value=100, step=50)

        fig = go.Figure()
        fig.add_trace(go.Scatter(y=y.values[:n], mode='lines+markers', name='R√©el'))
        fig.add_trace(go.Scatter(y=y_pred[:n], mode='lines+markers', name='Pr√©dit'))
        fig.update_layout(
            title="Comparaison R√©el vs Pr√©dit",
            xaxis_title="Index",
            yaxis_title="Valeur",
            hovermode="x unified"
        )
        st.plotly_chart(fig, use_container_width=True)

    except Exception as e:
        st.error(f"‚ùå Erreur lors de la pr√©diction : {e}")
